name: Performance Benchmarks

on:
  push:  # Run on every commit to any branch
  pull_request:  # Also run on PRs for comparison comments

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch full history for git operations

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Cache COCO annotations
        uses: actions/cache@v3
        with:
          path: benchmarks/coco_cache
          key: coco-annotations-${{ hashFiles('benchmarks/run_benchmarks.py') }}
          restore-keys: |
            coco-annotations-

      - name: Install dependencies
        run: |
          pip install maturin
          pip install -r benchmarks/requirements.txt

      - name: Build rust-nms (release mode)
        run: |
          maturin build --release
          pip install target/wheels/rust_nms-*.whl

      - name: Run benchmarks
        id: run_bench
        run: |
          cd benchmarks
          python run_benchmarks.py 2>&1 | tee benchmark_output.txt

      - name: Save benchmark results
        run: |
          cp benchmarks/latest_benchmark.json /tmp/current_benchmark.json

      # Performance regression check (runs on all commits)
      - name: Check for performance regression
        run: |
          # Get baseline from main branch
          git fetch origin main:main || true
          git show main:benchmarks/history.json > /tmp/main_history.json 2>/dev/null || echo "[]" > /tmp/main_history.json

          python3 << 'EOF'
          import json
          import sys

          IMPROVEMENT_THRESHOLD = 0.05  # Fail if NOT at least 5% faster

          # Load current benchmark
          with open('/tmp/current_benchmark.json', 'r') as f:
              current = json.load(f)

          # Load main history
          with open('/tmp/main_history.json', 'r') as f:
              main_history = json.load(f)

          if not main_history:
              print("âœ“ No baseline data available. Skipping performance check.")
              sys.exit(0)

          baseline = main_history[-1]

          # Check for improvements (grouped by category)
          print("Checking for performance improvements (required: â‰¥5% faster for NMS OR mask_to_polygons)...")
          print()

          nms_failures = []
          mask_failures = []

          for bench_name in sorted(current['benchmarks'].keys()):
              curr_data = current['benchmarks'][bench_name]
              base_data = baseline['benchmarks'].get(bench_name, {})

              if not base_data or 'time_ms' not in curr_data or 'time_ms' not in base_data:
                  continue

              curr_time = curr_data['time_ms']
              base_time = base_data['time_ms']
              change = (curr_time - base_time) / base_time
              improvement = -change  # Negative change = improvement (faster)

              status = "âœ“" if improvement >= IMPROVEMENT_THRESHOLD else "âœ—"
              print(f"{status} {bench_name}: {base_time:.2f}ms â†’ {curr_time:.2f}ms ({change*100:+.1f}%) [improvement: {improvement*100:.1f}%]")

              if improvement < IMPROVEMENT_THRESHOLD:
                  failure_info = {
                      'name': bench_name,
                      'baseline': base_time,
                      'current': curr_time,
                      'change_pct': change * 100,
                      'improvement_pct': improvement * 100
                  }

                  if bench_name.startswith('nms_'):
                      nms_failures.append(failure_info)
                  elif bench_name.startswith('mask_to_polygons_'):
                      mask_failures.append(failure_info)

          # Pass if either NMS or mask_to_polygons category improved by â‰¥5%
          nms_passed = len(nms_failures) == 0
          mask_passed = len(mask_failures) == 0

          print()
          print(f"NMS benchmarks: {'âœ… PASSED' if nms_passed else 'âœ— FAILED'} ({len(nms_failures)} benchmarks below threshold)")
          print(f"Mask benchmarks: {'âœ… PASSED' if mask_passed else 'âœ— FAILED'} ({len(mask_failures)} benchmarks below threshold)")

          if nms_passed or mask_passed:
              print()
              print("âœ… Performance check PASSED!")
              print("At least one category (NMS or mask_to_polygons) improved by â‰¥5%")
              sys.exit(0)
          else:
              print()
              print("=" * 60)
              print("âŒ INSUFFICIENT PERFORMANCE IMPROVEMENT")
              print("=" * 60)
              print("Either NMS or mask_to_polygons benchmarks must ALL be â‰¥5% faster.")
              print()

              if nms_failures:
                  print("NMS benchmarks that didn't improve enough:")
                  for fail in nms_failures:
                      print(f"  {fail['name']}: {fail['baseline']:.2f}ms â†’ {fail['current']:.2f}ms")
                      print(f"    Required: â‰¥5.0% faster, Actual: {fail['improvement_pct']:.1f}% {'faster' if fail['improvement_pct'] > 0 else 'slower'}")
                  print()

              if mask_failures:
                  print("Mask benchmarks that didn't improve enough:")
                  for fail in mask_failures:
                      print(f"  {fail['name']}: {fail['baseline']:.2f}ms â†’ {fail['current']:.2f}ms")
                      print(f"    Required: â‰¥5.0% faster, Actual: {fail['improvement_pct']:.1f}% {'faster' if fail['improvement_pct'] > 0 else 'slower'}")
                  print()

              print(f"Baseline commit: {baseline['commit'][:8]}")
              print(f"Current commit:  {current['commit'][:8]}")
              sys.exit(1)
          EOF

      # For pull requests: Compare with main branch
      - name: Compare with main (PR only)
        if: github.event_name == 'pull_request'
        run: |
          # Checkout main branch history.json
          git fetch origin main:main
          git show main:benchmarks/history.json > /tmp/main_history.json || echo "[]" > /tmp/main_history.json

          # Create comparison script
          python3 << 'EOF'
          import json
          import sys

          # Load current benchmark
          with open('/tmp/current_benchmark.json', 'r') as f:
              current = json.load(f)

          # Load main history
          with open('/tmp/main_history.json', 'r') as f:
              main_history = json.load(f)

          if not main_history:
              print("No baseline data available. This is the first benchmark run.")
              sys.exit(0)

          baseline = main_history[-1]  # Most recent main benchmark

          # Generate comparison report
          report = "## ðŸ“Š Performance Benchmark Results\n\n"
          report += f"**Baseline:** `{baseline['commit'][:8]}` (main)\n"
          report += f"**Current:** `{current['commit'][:8]}` ({current['branch']})\n\n"

          # Compare NMS benchmarks
          report += "### NMS Performance\n\n"
          report += "| Input Size | Baseline (ms) | Current (ms) | Change | Throughput |\n"
          report += "|------------|---------------|--------------|--------|------------|\n"

          for bench_name in sorted(current['benchmarks'].keys()):
              if not bench_name.startswith('nms_'):
                  continue

              curr_data = current['benchmarks'][bench_name]
              base_data = baseline['benchmarks'].get(bench_name, {})

              if not base_data:
                  continue

              curr_time = curr_data['time_ms']
              base_time = base_data['time_ms']
              change_pct = ((curr_time - base_time) / base_time) * 100

              # Format change with emoji
              if abs(change_pct) < 2:
                  emoji = "âž¡ï¸"
                  color = ""
              elif change_pct < 0:
                  emoji = "â¬‡ï¸"
                  color = ""  # Faster is better
              else:
                  emoji = "â¬†ï¸"
                  color = ""  # Slower is worse

              throughput = f"{curr_data['throughput_boxes_per_sec']/1000:.0f}k boxes/s"
              report += f"| {curr_data['n_boxes']:,} boxes | {base_time:.2f} | {curr_time:.2f} | {emoji} {change_pct:+.1f}% | {throughput} |\n"

          # Compare mask benchmarks
          report += "\n### Mask to Polygons Performance\n\n"
          report += "| Image Size | Baseline (ms) | Current (ms) | Change | Throughput |\n"
          report += "|------------|---------------|--------------|--------|------------|\n"

          for bench_name in sorted(current['benchmarks'].keys()):
              if not bench_name.startswith('mask_to_polygons_'):
                  continue

              curr_data = current['benchmarks'][bench_name]
              base_data = baseline['benchmarks'].get(bench_name, {})

              if not base_data:
                  continue

              curr_time = curr_data['time_ms']
              base_time = base_data['time_ms']
              change_pct = ((curr_time - base_time) / base_time) * 100

              # Format change with emoji
              if abs(change_pct) < 2:
                  emoji = "âž¡ï¸"
              elif change_pct < 0:
                  emoji = "â¬‡ï¸"
              else:
                  emoji = "â¬†ï¸"

              throughput = f"{curr_data['throughput_megapixels_per_sec']:.1f} MP/s"
              report += f"| {curr_data['size']} | {base_time:.2f} | {curr_time:.2f} | {emoji} {change_pct:+.1f}% | {throughput} |\n"

          report += "\n---\n"
          report += "*Benchmarks run on COCO train2017 dataset*\n"

          # Save report
          with open('/tmp/pr_comment.md', 'w') as f:
              f.write(report)

          print(report)
          EOF

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            let comment;
            try {
              comment = fs.readFileSync('/tmp/pr_comment.md', 'utf8');
            } catch (err) {
              console.log('No comparison report found, skipping comment');
              return;
            }

            // Find existing benchmark comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.data.find(c =>
              c.user.type === 'Bot' && c.body.includes('Performance Benchmark Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      # For main branch: Update history and regenerate charts
      - name: Update benchmark history (main only)
        if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
        run: |
          cd benchmarks

          # Append to history
          python3 << 'EOF'
          import json

          # Load current benchmark
          with open('latest_benchmark.json', 'r') as f:
              current = json.load(f)

          # Load history
          try:
              with open('history.json', 'r') as f:
                  history = json.load(f)
          except FileNotFoundError:
              history = []

          # Append current to history
          history.append(current)

          # Save updated history
          with open('history.json', 'w') as f:
              json.dump(history, f, indent=2)

          print(f"Added benchmark to history. Total entries: {len(history)}")
          EOF

          # Generate updated charts
          python generate_dashboard.py

          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Commit and push
          git add history.json nms_performance.png mask_performance.png
          git commit -m "Update performance benchmarks [skip ci]" || echo "No changes to commit"
          git push || echo "Nothing to push"
